{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Preparacion del Dataset](#preparaciondataset)\n",
    "\n",
    "    1.1 [Visualización de los datos](#visualizaciondatos) <br>\n",
    "    1.2 [Funciones Útiles](#fu) <br>\n",
    "    1.3 [Carga del Dataset](#cargadeldataset) <br>\n",
    "    1.4 [Modificación de imagenes a baja y media resolución](#modimgs) <br>\n",
    "    1.5 [Creación de los datasets](#creaciondatasets)\n",
    "    \n",
    "    - [a). Baja resolución (256x256)](#br) <br>\n",
    "    - [b). Media resolución (512x512)](#mr)<br>\n",
    "    - [c). Alta resolución (1024x1024)](#hr)\n",
    "2. [Creación y Entrenamiento de modelos](#creacionentrenamientomodelos)\n",
    "\n",
    "    2.1 [Autoencoder (Vanilla)](#ae) <br>\n",
    "    &emsp;[Introducción](#aei) <br>\n",
    "    &emsp;2.1.1 [Creación del Modelo](#aecm) <br>\n",
    "    &emsp;2.1.2 [Entrenamiento del Modelo](#aeem) <br>\n",
    "    &emsp;2.1.3 [Resultados del entrenamiento](#aer) <br>\n",
    "\n",
    "    2.2 [Variational Auto Encoder (VAE)](#vae) <br>\n",
    "    &emsp;[Introducción](#vaei) <br>\n",
    "    &emsp;2.2.1 [Creación del Modelo V1](#vaecm1) <br>\n",
    "    &emsp;2.2.2 [Entrenamiento del modelo V1](#vaeemv1) <br>\n",
    "    &emsp;2.2.3 [Resultados del entrenamiento V1](#vaerv1) <br>\n",
    "    &emsp;2.2.4 [Creación del Modelo V2](#vaecmv2) <br>\n",
    "    &emsp;2.2.5 [Entrenamiento del Modelo V2](#vaeemv2) <br>\n",
    "    &emsp;2.2.6 [Resultados del entrenamiento V2](#vaermv2) <br>\n",
    "    \n",
    "    2.3 [GAN](#gan) <br>\n",
    "    &emsp;[Introducción a las redes GAN](#gani) <br>\n",
    "    &emsp;2.3.1 [Declaración de los Generadores](#dgan) <br>\n",
    "    &emsp;2.3.2 [Declaración de las funciones de pérdida](#dfp) <br>\n",
    "    &emsp;2.3.3 [Declaración del discriminador](#dd) <br>\n",
    "    &emsp;2.3.4 [Declaracion de las funciones de Entrenamiento](#dfe) <br>\n",
    "    &emsp;2.3.5 [Entrenamiento y comprobación del modelo](#ganecm) <br>\n",
    "    \n",
    "    2.4 [GAN con U-Net o pix2pix](#cgan) <br>\n",
    "    &emsp;2.4.1 [Declaración de los Generadores](#dunet) <br>\n",
    "    &emsp;2.4.2 [Declaración de los discriminadores y entrenamiento](#unetde) <br>\n",
    "    \n",
    "    2.5 [EDSRGAN](#edsr) <br>\n",
    "    &emsp;2.5.1 [Creación del generador con arquitectura EDSR](#edsr) <br>\n",
    "    &emsp;2.5.2 [Declaración de los discriminadores y entrenamiento](#edsrde)<br>\n",
    "3. [Comparación de modelos](#comparacionmodelos)\n",
    "    \n",
    "    3.1 [Carga de modelos](#cm)<br>\n",
    "    3.2 [Comparativa de imágenes](#ci)<br>\n",
    "    3.3 [Tablas comparativas de métricas](#tcm) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "from tensorflow.keras import backend as bk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparación del Dataset <a id=\"preparaciondataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Visualización de los datos <a id=\"visualizaciondatos\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a abrir un par de imágenes para ver el contenido del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "show_image = Image.open(\"./Dataset/DIV2K_train_HR/0001.png\")\n",
    "show_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.imshow(show_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image = Image.open(\"./Dataset/DIV2K_train_HR/0124.png\")\n",
    "show_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.imshow(show_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, las imágenes tienen un tamaño constante de 2040 de ancho o de alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Funciones útiles: <a id=\"fu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función enseña las diferencias entre la imagen de input, la imagen real y la predicha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_diff(model,valid_dataset=None, input=None, target=None,figsize=(15,15),title=\"\"):\n",
    "\n",
    "    if valid_dataset is not None:\n",
    "        input, target = next(iter(valid_dataset))\n",
    "        input = input.numpy()\n",
    "        target = target.numpy()\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(input[0]) \n",
    "    plt.title('Input')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(target[0])  \n",
    "    plt.title('Ground Truth')\n",
    "    \n",
    "    s = model.predict(input,verbose=0)[0]\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(s)\n",
    "    plt.title('Prediction')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.subplots_adjust(top=1.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta funcion muestra un ejemplo del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(valid_dataset=None,input=None,target=None, dim=2):\n",
    "\n",
    "    if valid_dataset is not None:\n",
    "        if dim == 2:\n",
    "            input, target = next(iter(valid_dataset))\n",
    "            input = input.numpy()\n",
    "            target = target.numpy()\n",
    "        elif dim == 1:\n",
    "            input = next(iter(valid_dataset))\n",
    "            input = input.numpy()\n",
    "            target = input\n",
    "        else:\n",
    "            print(\"Invalid dim when passing a dataset\")\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(input[0]) \n",
    "    plt.title('Input')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(target[0])  \n",
    "    plt.title('Ground Truth')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Carga del Dataset <a id=\"cargadeldataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen algunas variables globales como las rutas de los directorios del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIV2K_TRAIN_HR_PATH = \"Dataset/DIV2K_train_HR/*.png\"\n",
    "DIV2K_VALID_HR_PATH = \"Dataset/DIV2K_valid_HR/*.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se está haciendo uso del dataset DIV2k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Modificación de imagenes a baja y media resolución <a id=\"modimgs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Esta función añade desenfoque gaussiano dependiendo del tamaño del kernel y del valor sigma:\n",
    " (Nota: obtenido de: https://gist.github.com/blzq/c87d42f45a8c5a53f5b393e27b1f5319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_blur(img, kernel_size=2, sigma=50):\n",
    "    def gauss_kernel(channels, kernel_size, sigma):\n",
    "        ax = tf.range(-kernel_size // 2 + 1.0, kernel_size // 2 + 1.0)\n",
    "        xx, yy = tf.meshgrid(ax, ax)\n",
    "        kernel = tf.exp(-(xx ** 2 + yy ** 2) / (2.0 * sigma ** 2))\n",
    "        kernel = kernel / tf.reduce_sum(kernel)\n",
    "        kernel = tf.tile(kernel[..., tf.newaxis], [1, 1, channels])\n",
    "        return kernel\n",
    "\n",
    "    gaussian_kernel = gauss_kernel(tf.shape(img)[-1], kernel_size, sigma)\n",
    "    gaussian_kernel = gaussian_kernel[..., tf.newaxis]\n",
    "\n",
    "    img = tf.expand_dims(img, axis=0)  \n",
    "    img_blurred = tf.nn.depthwise_conv2d(img, gaussian_kernel, [1, 1, 1, 1],\n",
    "                                          padding='SAME', data_format='NHWC')\n",
    "    img_blurred = tf.squeeze(img_blurred, axis=0)  \n",
    "\n",
    "    return img_blurred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se añade una máscara y se define la cantidad de ruido y lo grande que es la máscara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_part_of_image(image, noise_level, corruption_level=0.04):\n",
    "    \n",
    "    mask = tf.random.uniform(tf.shape(image)[:2]) < corruption_level\n",
    "    noise = tf.random.normal(tf.shape(image), stddev=noise_level)\n",
    "    noise = (noise + 1.0) / 2.0\n",
    "    corrupted_image = tf.where(mask[..., tf.newaxis], tf.clip_by_value(image + noise, 0.0, 1.0), image)\n",
    "    \n",
    "    return corrupted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que añade ruido y desenfoque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_jitter(img,noise=0.1,corrupt=0.04,blur=2):\n",
    "    img = gaussian_blur(img,kernel_size=blur)\n",
    "    img = corrupt_part_of_image(img,noise_level=noise,corruption_level=corrupt)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para cargar una imagen png y transformarla a float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(input_path):\n",
    "    img = tf.io.read_file(input_path)\n",
    "    img = tf.cast(tf.image.decode_png(img,channels=3),tf.float32)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para normalizar la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    #img = (img / 127.5) - 1 # Normalización de la imagen entre [-1 y 1]\n",
    "    img = img / 255 #Normalización de la imagen entre [0 y 1]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para crear una canalización de datos y procesar las imágenes de alta resolución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target = Ground Truth ; Input = Imagen que se va a meter al modelo\n",
    "\n",
    "# Salida: 256 y 1024\n",
    "def low_res_img_map(input_path):\n",
    "    target_img = load(input_path)\n",
    "    target_img = tf.image.resize(target_img,size=(1024,1024))\n",
    "    \n",
    "    input_image = tf.image.resize(target_img,size=(256,256))\n",
    "\n",
    "    input_image = random_jitter(input_image)\n",
    "\n",
    "    target_img = normalize(target_img)\n",
    "    input_image = normalize(input_image)\n",
    "\n",
    "    return input_image, target_img\n",
    "\n",
    "# Salida: 512 y 1024\n",
    "def med_res_img_map(input_path):\n",
    "    target_img = load(input_path)\n",
    "    target_img = tf.image.resize(target_img,size=(1024,1024))\n",
    "    \n",
    "    input_image = tf.image.resize(target_img,size=(512,512))\n",
    "\n",
    "    input_image = random_jitter(input_image,corrupt=0.1)\n",
    "\n",
    "    target_img = normalize(target_img)\n",
    "    input_image = normalize(input_image)\n",
    "\n",
    "    return input_image, target_img\n",
    "\n",
    "# Saalda: 1024 y 1024\n",
    "def high_res_img_map(input_path):\n",
    "    target_img = load(input_path)\n",
    "    target_img = tf.image.resize(target_img,size=(1024,1024))\n",
    "\n",
    "    input_image = random_jitter(target_img,noise=0.4,corrupt=0.2,blur=8)\n",
    "\n",
    "    target_img = normalize(target_img)\n",
    "    input_image = normalize(input_image)\n",
    "\n",
    "    return input_image, target_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Creación de los datasets <a id=\"creaciondatasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a crear una canalización para que las imágenes se procesen en la cpu en tiempo de entrenamiento y no en GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_path, res_func, batch_size=1):\n",
    "    dataset = tf.data.Dataset.list_files(str(dataset_path))\n",
    "    dataset = dataset.map(res_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a). Baja resolución (256x256) <a id=\"br\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_train_dataset = create_dataset(DIV2K_TRAIN_HR_PATH,low_res_img_map)\n",
    "low_res_valid_dataset = create_dataset(DIV2K_VALID_HR_PATH,low_res_img_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plot_dataset(low_res_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b). Media resolución (512x512) <a id=\"mr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_res_train_dataset = create_dataset(DIV2K_TRAIN_HR_PATH,med_res_img_map)\n",
    "med_res_valid_dataset = create_dataset(DIV2K_VALID_HR_PATH,med_res_img_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(med_res_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c). Alta resolución (1024x1024) <a id=\"hr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_res_train_dataset = create_dataset(DIV2K_TRAIN_HR_PATH,high_res_img_map)\n",
    "high_res_valid_dataset = create_dataset(DIV2K_VALID_HR_PATH,high_res_img_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(high_res_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creación y Entrenamiento de modelos <a id=\"creacionentrenamientomodelos\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueba que se detecta la tarjeta gráfica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Autoencoder (Vanilla) <a id=\"ae\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción <a id=\"aei\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un autoencoder es un tipo de red de neuronas capaz de aprender características de los datos, reducir su dimensionalidad y reconstruir estos mismos manteniendo parte de su fidelidad. Es decir, consta de tres partes: *codificador*, *espacio latente* y *decodificador*.\n",
    "\n",
    "El *codificador* o en ingles *encoder* se puede representar de la siguiente forma:\n",
    "\n",
    "$h_i = g(X_i)$ , donde $h_i ∈ R^q$ que simboliza el *espacio latente*;\n",
    "\n",
    "El *decodificador* o en ingles *decoder* se puede representar de la siguiente forma:\n",
    "\n",
    "$\\tilde{x}_i = f(h_i) = f(g(x_i))$ , donde $\\tilde{x}_i ∈ R^n$ ;\n",
    "\n",
    "Para el entrenamiento del autoencoder, hay que buscar las funciones f y g que minimicen la diferencia entre $x_i$ y $\\tilde{x}_i$:\n",
    "\n",
    "$argmin_{f,g} <[∆(x_i,f(g(x_i)))]>$ ;\n",
    "\n",
    "$<·>$, Indica media de todas las desviaciones observadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Auto Encoder Vanilla](./Images/latent_representation_AE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Creación del Modelo <a id=\"aecm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a crear una clase autoencoder un tanto diferente, para este autoencoder se van ha utilizar entradas de datos diferentes a la salida. Se va ha intentar reescalar las imágenes de 256x256 y 512x512 a 1024x1024, añadiendo 2 y 1 capas más de convolución respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.models.Model):\n",
    "    def __init__(self,input_shape):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Codificador\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=input_shape),\n",
    "            tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        ])\n",
    "\n",
    "        #Decodificador\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.UpSampling2D((2, 2)),\n",
    "            tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.UpSampling2D((2, 2)),\n",
    "            tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.UpSampling2D((2, 2)),\n",
    "        ])\n",
    "\n",
    "        #Si la imagen tiene un tamaño de 1024 la salida son 3 filtros (RGB)\n",
    "        if input_shape[0] == 1024:\n",
    "            self.decoder.add(tf.keras.layers.Convolution2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "        \n",
    "        #Si la imagen tiene un tamaño de 512 la salida es una capa convolucional mas 3 filtros (RGB)\n",
    "        if input_shape[0] == 512:\n",
    "            self.decoder.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding='same'))\n",
    "            self.decoder.add(tf.keras.layers.UpSampling2D((2, 2)))\n",
    "            self.decoder.add(tf.keras.layers.Convolution2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "\n",
    "        #Si la imagen tiene un tamaño de 256 la salida son dos capas convolucionales mas 3 filtros (RGB)\n",
    "        if input_shape[0] == 256:\n",
    "            self.decoder.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding='same'))\n",
    "            self.decoder.add(tf.keras.layers.UpSampling2D((2, 2)))\n",
    "            self.decoder.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu', padding='same'))\n",
    "            self.decoder.add(tf.keras.layers.UpSampling2D((2, 2)))\n",
    "            self.decoder.add(tf.keras.layers.Convolution2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Entrenamiento del modelo <a id=\"aeem\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se declaran los tres modelos: 256,512 y 1024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_256 = Autoencoder(input_shape=(256,256,3))\n",
    "autoencoder_512 = Autoencoder(input_shape=(512,512,3))\n",
    "autoencoder_1024 = Autoencoder(input_shape=(1024,1024,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena cada red con pérdida MSE y con el optimizador ADAM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mira la pérdida del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_1024.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history_high_res = autoencoder_1024.fit(high_res_train_dataset, epochs=25,validation_data=high_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_high_res.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_512.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history_mid_res = autoencoder_512.fit(med_res_train_dataset, epochs=30,validation_data=med_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_mid_res.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_256.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history_low_res = autoencoder_256.fit(low_res_train_dataset, epochs=40,validation_data=low_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_low_res.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guardan los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_256.save(\"Checkpoints/Autoencoder/autoencoder_256_model\")\n",
    "autoencoder_512.save(\"Checkpoints/Autoencoder/autoencoder_512_model\")\n",
    "autoencoder_1024.save(\"Checkpoints/Autoencoder/autoencoder_1024_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Resultados del entrenamiento: <a id=\"aer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "256->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(autoencoder_256,low_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(autoencoder_512,med_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1024->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(autoencoder_1024,high_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar los resultados no son para nada malos, pero pueden mejorarse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Variational Auto Encoder (VAE) <a id=\"vae\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción: <a id=\"vaei\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo nuestro autoencoder ya creado, lo que se busca con un variational autoencoder es, con la misma estructura, intentar regularizar mejor los datos como se ve en el gráfico: <br><br>\n",
    "![vae](./Images/variational_autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siendo $x∈X^D$ un vector de variables observables, donde $X\\sube R$ o $X\\sube Z$ donde $Z\\in R^M$ sea un vector de variables latentes. Las variables latentes son variables que están dentro del modelo, osea que no forman parte del dataset. Para representar en forma de red Bayesiana o modelos gráficos dirigidos se necesita la probabilidad conjunta de x y z: $p_\\theta(x,z)dz$. \n",
    "\n",
    "Para sacar la similitud marginal o la evidencia con las propiedades de la probabilidad conjunta para datos continups se tiene: \n",
    "\n",
    "$p_\\theta(x)=\\int {p_\\theta(x,z)dz}$. \n",
    "\n",
    "Para entrenar el modelo es necesario despejar z cosa que es computacionalmente imposible tratar este tipo de problema así que se opta por otra solución y es intentar maximizar la función ELBO (evidence lower bound):  \n",
    "\n",
    "(ELBO) => $\\ln{p_\\theta(x)} \\le E_{z\\text{\\textasciitilde}q_\\phi(z/x)}[\\ln{p_\\theta(x/z)} + \\ln{p_\\theta(z)} - \\ln{q_\\phi(z/x)}]$\n",
    "\n",
    "- Donde $q_\\phi(z/x)=N(\\mu_\\phi(x),\\sigma_\\phi^2(x)I)$ es el *codificador*\n",
    "- Donde $p_\\theta(x/z)=N(\\mu,\\varSigma)$ es el *decodificador*\n",
    "- Donde $p_\\lambda(z)=N(0,1)$ es el *prior* o la *similitud marginal*\n",
    "\n",
    "![vae_model](./Images/variational_autoencoder_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma mucho más útil de escribir el ELBO es agrupar la parte del *prior* y del *decodificador* convirtiéndolo en una divergencia *KL(Kullback-Leibler)*\n",
    "\n",
    "- $\\ln{p_\\theta(x)} \\le E_{z\\text{\\textasciitilde}q_\\phi(z/x)}[\\ln{p_\\theta(x/z)} + \\textcolor{#FF33F3}{\\ln{p_\\theta(z)}} - \\textcolor{#FF33F3}{\\ln{q_\\phi(z/x)}}] =$\n",
    "- $ = -\\textcolor{#FF33F3}{KL(q_\\phi(z/x)||p(z))} + E_{z\\text{\\textasciitilde}q_\\phi(z/x)}[\\ln{p_\\theta(x/z)}]$\n",
    "\n",
    "La divergencia *KL(Kullback-Leibler)* es una medida de la diferencia entre dos distribuciones de probabilidad y se escribe así: \n",
    "\n",
    "- $D_{kl}(P||Q)=\\sum_xP(x)log \\frac {P(x)} {Q(x)}$ para valores continuos y discretos.\n",
    "\n",
    "Esto es útil cuando el valor de KL es fácil de analizar siendo: \n",
    "\n",
    "- $-\\textcolor{#FF33F3}{KL(q_\\phi(z/x)||p(z))}= \\textcolor{#DAF7A6}{\\frac {1}{2}(1 + \\ln\\sigma_\\phi^2(x) - \\sigma_\\phi^2(x) - \\mu_\\phi(x)^2)}$  \n",
    "\n",
    "Siendo este valor el valor de pérdida del modelo al que se le va ha añadir el error l2 con un factor de balance B para que sea mas fiel la salida. Entonces, el error del modelo será: \n",
    "\n",
    "- $loss = B*l2(x,(x)´) + (\\textcolor{#DAF7A6}{\\frac {1}{2}(1 + \\ln\\sigma_\\phi^2(x) - \\sigma_\\phi^2(x) - \\mu_\\phi(x)^2)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo, además, se va ha entrenar con *SGD* o *descenso del gradiente estocástico*, con lo que se llama *reparametrization trick*, que busca poder diferenciar las varaibles aleatorias continuas respecto de sus parámetros. En el caso del VAE, tenemos un vector de medias $\\mu_\\theta(x)$ y el vector de desviaciones típicas $\\sigma_\\theta^2(x)$. En lugar de muestrear directamente esta distribución gaussiana $(\\mu,\\sigma)$, se puede muestrear una distribución gaussiana estándar  $N(0,1)$, y luego transformar este muestreo mediante la siguiente fórmula:\n",
    "\n",
    "- $z = \\mu + \\sigma \\bigodot \\epsilon$ ; Donde $\\bigodot$ denota la multiplicación de cada elemento por $\\epsilon$, que es una distribución gaussiana estandar (ruido)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Creación del modelo V1 <a id=\"vaecm1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una primera aproximación se va a crear un VAE con convolucionales que coja imágenes de 1024x1024x3 y las intente recrear. Para ello, se va a crear una canalización de datos que coja las imágenes del dataset y las transforme a 1024x1024: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_map(input_path):\n",
    "    \n",
    "    target_img = load(input_path)\n",
    "    target_img = tf.image.resize(target_img,size=(1024,1024))\n",
    "    target_img = normalize(target_img)\n",
    "\n",
    "    return target_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_train_dataset = create_dataset(DIV2K_TRAIN_HR_PATH,vae_map,batch_size=2)\n",
    "vae_valid_dataset = create_dataset(DIV2K_VALID_HR_PATH,vae_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la creación del modelo, se utilizan capas convolucionales que disminuyen el tamaño de la imagen para poder evitar errores de memoria (OOM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CVAE(latent_dim):\n",
    "\n",
    "  #Encoder\n",
    "  encoder_input = tf.keras.layers.Input(shape=(1024,1024,3))\n",
    "  \n",
    "  x = tf.keras.layers.Conv2D(filters=16,  kernel_size=3, activation='relu', padding='same')(encoder_input) # 512x512x16\n",
    "  x = tf.keras.layers.Conv2D(filters=16,  kernel_size=3, strides=2, activation='relu', padding='same')(x) # 512x512x16\n",
    "  x = tf.keras.layers.Conv2D(filters=32,  kernel_size=3, strides=2, activation='relu', padding='same')(x) # 256x256x32\n",
    "  x = tf.keras.layers.Conv2D(filters=64,  kernel_size=3, strides=2, activation='relu', padding='same')(x) # 128x128x64\n",
    "  x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')(x) # 64x64x128\n",
    "  x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2 ,activation='relu', padding='same')(x) # 32x32x256\n",
    "\n",
    "  s = bk.int_shape(x)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  \n",
    "  z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
    "  z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
    "\n",
    "  @tf.function\n",
    "  def sampling(args):\n",
    "      z_mean, z_log_var = args\n",
    "      epsilon = bk.random_normal(shape=(bk.shape(z_mean)[0], latent_dim)) # Vec\n",
    "      return z_mean + bk.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "  # Reparameterization trick\n",
    "  z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "  \n",
    "  encoder = tf.keras.Model(encoder_input,[z_mean,z_log_var,z], name=\"encoder\")\n",
    "\n",
    "  #Decoder\n",
    "  decoder_input = tf.keras.layers.Input(shape=bk.int_shape(z)[1:])\n",
    "  x = tf.keras.layers.Dense(np.prod(s[1:]), activation='relu')(decoder_input)\n",
    "  x = tf.keras.layers.Reshape(s[1:])(x)\n",
    "  \n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')(x) # 64x64x256\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(x) # 128x128x128\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3,  strides=2, padding='same', activation='relu')(x) # 256x256x64\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3,  strides=2,padding='same', activation='relu')(x) # 512x512x32\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=3,  strides=2,padding='same', activation='relu')(x) # 1024x1024x16\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=16, kernel_size=3 ,padding='same', activation='relu')(x) # 1024x1024x16\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=1, activation='sigmoid', padding='same')(x) # 1024x1024x3\n",
    "\n",
    "  decoder = tf.keras.models.Model(decoder_input,x, name=\"decoder\")\n",
    "  decoder_output = decoder(encoder(encoder_input)[2])\n",
    "\n",
    "  #Creación del modelo\n",
    "  vae = tf.keras.models.Model(encoder_input, decoder_output, name = \"vae\")\n",
    "\n",
    "  #Valores de pérdida\n",
    "  reconstruction_loss = tf.keras.losses.mse(bk.flatten(encoder_input), bk.flatten(decoder_output))\n",
    "  reconstruction_loss *= 1024 * 1024 * 3\n",
    "  kl_loss = -0.5 * bk.sum(1 + z_log_var - bk.square(z_mean) - bk.exp(z_log_var), axis=1)\n",
    "  B = 1000   \n",
    "  vae_loss = bk.mean(B * reconstruction_loss + kl_loss)\n",
    "  vae.add_loss(vae_loss)\n",
    "  vae.add_metric(kl_loss, name=\"kl_loss\")\n",
    "  vae.add_metric(reconstruction_loss, name=\"reconstruction_loss\")\n",
    "  vae.compile(optimizer='adam')\n",
    "\n",
    "  return vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye el modelo con un espacio latente de 256:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprueba la estructura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(cvae, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Entrenamiento del modelo V1 <a id=\"vaeemv1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el modelo con 500 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_history = cvae.fit(vae_train_dataset,shuffle=True,epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guarda el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.save(\"Checkpoints/VAE/VAE_1024_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Resultados de entrenamiento <a id=\"vaerv1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una función para mostrar las diferencias entre la entrada y la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def plot_model_diff_vae(model,valid_dataset):\n",
    "\n",
    "    input  = next(iter(valid_dataset))\n",
    "\n",
    "    input = input.numpy()\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(input[0]) \n",
    "    plt.title('Input')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.imshow(model.predict(input, verbose=0)[0]) \n",
    "    plt.title('Prediction')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mustran las imágenes, una del conjunto de validación y la otra del conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff_vae(cvae,vae_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a ver como predice una imagen del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff_vae(cvae,vae_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, las imágenes no llegan a ser lo más fieles posibles a la imagen original. La razón principal es que, debido al tamaño de las imágenes, es necesario un espacio latente mucho más grande. El problema es que, si se aumenta el espacio latente, las dimensiones del modelo crecen exponencialmente, lo que lleva a problemas con la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Creación del modelo V2 <a id=\"vaecmv2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar otra solución: Separar el dataset en parches de 64x64x3 y así aumentar el espacio latente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que, pasándole un dataset y un tamaño de parche, extrae cada parche y lo convierte en una canalización de dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extract_patches(images_dataset, patch_size):\n",
    "\n",
    "    patch_height, patch_width, _ = patch_size\n",
    "    strides = [1, patch_height, patch_width, 1]\n",
    "\n",
    "    def extract_patches_from_image(image):\n",
    "\n",
    "        patches = tf.image.extract_patches(images=image,\n",
    "                                           sizes=[1, patch_height, patch_width, 1],\n",
    "                                           strides=strides,\n",
    "                                           rates=[1, 1, 1, 1],\n",
    "                                           padding='VALID')\n",
    "\n",
    "        patches = tf.reshape(patches, (-1, patch_height, patch_width, 3))\n",
    "        return patches\n",
    "\n",
    "    patches_dataset = images_dataset.map(extract_patches_from_image,\n",
    "                                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    # Concatena los parches\n",
    "    patches_dataset = patches_dataset.unbatch()\n",
    "    \n",
    "    return patches_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vae_train_dataset_patches = extract_patches(vae_train_dataset, (64,64,3)).batch(30)\n",
    "vae_valid_dataset_patches = extract_patches(vae_valid_dataset, (64,64,3)).batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestran los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plot_dataset(vae_train_dataset_patches,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación del modelo. En esta ocasión se ha optado por reducir la dimensionalidad del modelo ya que las imágenes son mucho más pequeñas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def CVAE_PATCH(latent_dim):\n",
    "\n",
    "  #Encoder\n",
    "  encoder_input = tf.keras.layers.Input(shape=(64,64,3))\n",
    "  \n",
    "  x = tf.keras.layers.Conv2D(filters=32,  kernel_size=3,  activation='relu', padding='same')(encoder_input) # 64\n",
    "  x = tf.keras.layers.Conv2D(filters=64,  kernel_size=3,  activation='relu', padding='same')(x) # 64\n",
    "  x = tf.keras.layers.Conv2D(filters=128,  kernel_size=3, strides=2, activation='relu', padding='same')(x) # 32\n",
    "  x = tf.keras.layers.Conv2D(filters=256, kernel_size=3,  activation='relu', padding='same')(x) # 32\n",
    "\n",
    "  s = bk.int_shape(x)\n",
    "  x = tf.keras.layers.Flatten()(x)\n",
    "  \n",
    "  z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
    "  z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
    "\n",
    "  @tf.function\n",
    "  def sampling(args):\n",
    "      z_mean, z_log_var = args\n",
    "      epsilon = bk.random_normal(shape=(bk.shape(z_mean)[0], latent_dim))\n",
    "      return z_mean + bk.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "  # Reparameterization trick\n",
    "  z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "  \n",
    "  encoder = tf.keras.Model(encoder_input,[z_mean,z_log_var,z], name=\"encoder\")\n",
    "\n",
    "  #Decoder\n",
    "  decoder_input = tf.keras.layers.Input(shape=bk.int_shape(z)[1:])\n",
    "  x = tf.keras.layers.Dense(np.prod(s[1:]), activation='relu')(decoder_input)\n",
    "  x = tf.keras.layers.Reshape(s[1:])(x)\n",
    "  \n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=3, padding='same', activation='relu')(x) # 32\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=1, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "  decoder = tf.keras.models.Model(decoder_input,x, name=\"decoder\")\n",
    "  decoder_output = decoder(encoder(encoder_input)[2])\n",
    "\n",
    "  vae = tf.keras.models.Model(encoder_input, decoder_output, name = \"vae\")\n",
    "\n",
    "  reconstruction_loss = tf.keras.losses.mse(bk.flatten(encoder_input), bk.flatten(decoder_output))\n",
    "  reconstruction_loss *= 64 * 64 * 3\n",
    "  kl_loss = -0.5 * bk.sum(1 + z_log_var - bk.square(z_mean) - bk.exp(z_log_var), axis=1)\n",
    "  B = 1000   \n",
    "  vae_loss = bk.mean(B * reconstruction_loss + kl_loss)\n",
    "  vae.add_loss(vae_loss)\n",
    "  vae.add_metric(kl_loss, name=\"kl_loss\")\n",
    "  vae.add_metric(reconstruction_loss, name=\"reconstruction_loss\")\n",
    "  vae.compile(optimizer='adam')\n",
    "\n",
    "  return vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se coge un espacio latente de 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "cvae_patch = CVAE_PATCH(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Entrenamiento del modelo <a id=\"vaeemv2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_history = cvae_patch.fit(vae_train_dataset_patches,shuffle=True,epochs=15,validation_data=vae_valid_dataset_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "cvae_patch.save(\"Checkpoints/VAE_PATCH/VAE_128_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra el entrnamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cvae_history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Resultados del entrenamiento V2 <a id=\"vaermv2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestran los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plot_model_diff_vae(cvae_patch,vae_valid_dataset_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff_vae(cvae_patch,vae_train_dataset_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, las imágenes pierden un poco el color y dejan de estar tan pixelados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va ha probar a restaurar una imagen entera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que pasándole un modelo y un tensor de imagen, separa en parches la imagen, predice cada parche y la reconstruye mostrándola por pantalla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_patch_reconstruct(model, single_image,title=\"\",figsize=(15, 15)):\n",
    "\n",
    "    # Divide la imagen en parches de 48x48x3\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=single_image,  \n",
    "        sizes=[1, 64, 64, 1], \n",
    "        strides=[1, 64, 64, 1], \n",
    "        rates=[1, 1, 1, 1], \n",
    "        padding='VALID'  \n",
    "    )\n",
    "\n",
    "    patches = tf.reshape(patches, [-1, 64, 64, 3])\n",
    "\n",
    "    #Se predicen los parches:\n",
    "    reconstructed_patches = model.predict(patches,verbose=0)\n",
    "\n",
    "    reconstructed_image = np.zeros((1024, 1024, 3))\n",
    "\n",
    "    # Coloca cada parche en la imagen reconstruida\n",
    "    patch_size = 64\n",
    "    num_patches_per_row = 1024 // patch_size\n",
    "    for i in range(len(reconstructed_patches)):\n",
    "        row = i // num_patches_per_row\n",
    "        col = i % num_patches_per_row\n",
    "        reconstructed_image[row*patch_size:(row+1)*patch_size, col*patch_size:(col+1)*patch_size, :] = reconstructed_patches[i]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(single_image.numpy()[0]) \n",
    "    plt.title('Input')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.imshow(reconstructed_image) \n",
    "    plt.title('Prediction')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.subplots_adjust(top=1.3)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comparan las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image = vae_valid_dataset.take(1)\n",
    "single_image = next(iter(single_image))\n",
    "\n",
    "vae_patch_reconstruct(cvae_patch,single_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se prueba una imagen con ruido para ver cómo de bien quita el ruido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target = next(iter(high_res_valid_dataset))\n",
    "\n",
    "vae_patch_reconstruct(cvae_patch,input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ya observar, el modelo VAE (siendo entrenado de forma normal sin alterar la imagen de entrada como se ha hecho en el autoencoder), quita un poco de ruido pero no elimina la mayoría. Además, altera el color de las imágenes (quitando tonos verdes y azules) y haciendo que estas dejen de ser muy fiables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 GAN (Sin U-Net) <a id=\"gan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción a las redes GAN <a id=\"gani\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes GAN *(Generative Adversarial Networks)* son un tipo de arquitectura de red de neuronas. Este tipo de arquitectura consta de dos partes: la primera parte es el *generador* que, como su nombre indica, se encarga de generar imágenes y la segunda es el discriminador que se encarga de diferenciar si las imágenes son reales o no. Estas dos redes compiten por obtener los mejores resultados. El concepto de estas redes es parecido al algoritmo minimax, el discriminador y el generador intentan minimizar su valor de pérdida y maximizar la de su oponente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basic Gan](./Images/Basic_GAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fórmula general de las redes GAN parte de la fórmula de la entropía cruzada binaria  o *binary cross entropy*:<br><br>\n",
    "- $L = - \\varSigma \\space yln(\\widehat{y}) + (1-y)*ln(1-\\widehat{y})$\n",
    "\n",
    "- Si $y = 1 ; \\space \\widehat{y} = D(x) => L = ln[D(x)]$<br>\n",
    "\n",
    "- Si $y = 0 ; \\space \\widehat{y} = D(G(z)) => L = ln[1 - D(G(z))]$ <br>\n",
    "\n",
    "- Entonces: $L = ln[D(x)] + ln[1 - D(G(z))]$\n",
    "\n",
    "Si aplicamos la esperanza en toda la igualdad nos queda:\n",
    "\n",
    "- $E(L) = E(ln[D(x)]) + E(ln[1-D(G(z))])$\n",
    "\n",
    "Sabiendo las propiedades de la esperanza para valores Continuos:\n",
    "\n",
    "- $E[X] = \\int_{R} Xf(X)dx$ => $ \\textcolor{#FF33F3}{\\int P_{datos}(x)} (ln[D(x)])\\textcolor{#FF33F3}{dx} + \\textcolor{#FF33F3}{\\int P_{z}(z)}(ln[1-D(G(z))])\\textcolor{#FF33F3}{dz}$\n",
    "\n",
    "Las expresiones resaltadas se convierten en la función de valor:\n",
    "\n",
    "- $min_G max_D V(G,D) = \\textcolor{#FF33F3}{E_{x\\text{\\textasciitilde}P_{datos}}}[ln(D(x))] + \\textcolor{#FF33F3}{E_{z\\text{\\textasciitilde}Pz}}[ln(1-D(G(z)))]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Declaración de los Generadores <a id=\"dgan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se declaran las funciones de downsample y upsample que se utilizarán mas adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso de los generadores siguen la siguiente estructura:\n",
    "\n",
    "![generador normal](./Images/Generador_normal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator_256():\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "\n",
    "  #Downsample\n",
    "  x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same', activation='relu')(inputs) #128\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same', activation='relu')(x) # 32\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu')(x) # 16\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  #Upsample\n",
    "  x = tf.keras.layers.Conv2DTranspose(512, 4, strides=2, padding='same', activation='relu')(x) # 32\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same', activation='relu')(x) # 128\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu')(x) # 256\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(32, 4, strides=2, padding='same', activation='relu')(x) # 512\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(16, 4, strides=2, padding='same', activation='relu')(x) # 1024\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(3, 4, strides=1, padding='same', activation='sigmoid')(x) # 1024\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_256 = Generator_256()\n",
    "tf.keras.utils.plot_model(generator_256, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator_512():\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[512, 512, 3])\n",
    "\n",
    "  #Downsample\n",
    "  x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same', activation='relu')(inputs) #256\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same', activation='relu')(x) # 128\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu')(x) # 32\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  #Upsample\n",
    "  x = tf.keras.layers.Conv2DTranspose(512, 4, strides=2, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same', activation='relu')(x) # 128\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same', activation='relu')(x) # 256\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu')(x) # 512\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(32, 4, strides=2, padding='same', activation='relu')(x) # 512\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(3, 4, strides=1, padding='same', activation='sigmoid')(x) # 1024\n",
    "\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_512 = Generator_512()\n",
    "tf.keras.utils.plot_model(generator_512, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator_1024():\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[1024, 1024, 3])\n",
    "\n",
    "  #Downsample\n",
    "  x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same', activation='relu')(inputs) #512\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same', activation='relu')(x) # 256\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same', activation='relu')(x) # 128\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu')(x) # 64\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  #Upsample\n",
    "  x = tf.keras.layers.Conv2DTranspose(512, 4, strides=2, padding='same', activation='relu')(x) # 128\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same', activation='relu')(x) # 256\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same', activation='relu')(x) # 512\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu')(x) # 512\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Conv2DTranspose(3, 4, strides=1, padding='same', activation='sigmoid')(x) # 10224\n",
    "\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_1024 = Generator_1024()\n",
    "tf.keras.utils.plot_model(generator_1024, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Declaración de las funciones de pérdida <a id=\"dfp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la función de pérdida del generador es de la siguiente manera:\n",
    "\n",
    "![Generator_loss](Images\\losses_GAN_Generador.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(loss_object,disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # Mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la pérdida del Discriminador es de la siguiente manera:\n",
    "\n",
    "![Discriminator loss](Images\\losses_GAN_Discriminador.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(loss_object,disc_real_output, disc_generated_output):\n",
    "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Declarción del discriminador <a id=\"dd\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estructura del discriminador es un PatchGAN convolucional como en el artículo de Pix2Pix:\n",
    "\n",
    "1. El discriminador recoge dos entradas y las concatena.\n",
    "2. Hace cuatro downsample con Conv2d->BatchNorm->LeakyRelu cada uno.\n",
    "3. Se amplia y se aumenta el tamaño de la capa y se hace un ultimo Conv2d->BatchNorm->Leaky Relu y se vuelve a reducir.\n",
    "4. La capa final tiene un solo filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[1024, 1024, 3], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[1024, 1024, 3], name='target_image')\n",
    "\n",
    "  #Se concatenan las dos entradas\n",
    "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 1024, 1024, channels*2) \n",
    "\n",
    "  down1 = downsample(64, 4, False)(x)  # (batch_size, 512, 512, 64)\n",
    "  down2 = downsample(128, 4)(down1)  # (batch_size, 256, 256, 128)\n",
    "  down3 = downsample(256, 4)(down2)  # (batch_size, 128, 128, 256)\n",
    "  down4 = downsample(512, 4)(down3)  # (batch_size, 64, 64, 512)\n",
    "\n",
    "  # Se añade relleno\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4)  # (batch_size, 66, 66, 512)\n",
    "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 63, 63, 1024)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 65, 65, 1024)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 62, 62, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_256 = Discriminator()\n",
    "discriminator_512 = Discriminator()\n",
    "discriminator_1024 = Discriminator()\n",
    "\n",
    "tf.keras.utils.plot_model(discriminator_256, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Declaración de las funciones de entenamiento <a id=\"dfe\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el entrenamiento se entrena primero el generador y luego el discriminador. Estos NO pueden entrenarse a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(generator,discriminator,generator_optimizer,discriminator_optimizer,loss_object,input_image, target):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    \n",
    "    #Entrenamiento del generador\n",
    "    gen_output = generator(input_image, training=True)\n",
    "\n",
    "    # Uso el método bilinear para reescalar la imagen de input\n",
    "    bilinear_input_image = tf.image.resize(input_image,size=(1024,1024),method=\"bilinear\")\n",
    "\n",
    "    #Entrenamiento del Discriminador\n",
    "    disc_real_output = discriminator([bilinear_input_image, target], training=True)\n",
    "    disc_generated_output = discriminator([bilinear_input_image, gen_output], training=True)\n",
    "\n",
    "    #Se genera la pérdida total del generador y del discriminador:\n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(loss_object, disc_generated_output, gen_output, target)\n",
    "    disc_loss = discriminator_loss(loss_object, disc_real_output, disc_generated_output)\n",
    "\n",
    "  #Se aplican los gradientes a las redes\n",
    "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "  #Se aplican los gradientes a los optimizers\n",
    "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la funcion de entrenamiento en cada step se saca un batch de imágenes y se prepara el train_step. Cada 5k steps se guarda el checkpoint para poder restaurar la imagen cuando se desee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(generator ,discriminator, train_ds,generator_optimizer, discriminator_optimizer , checkpoint, checkpoint_prefix,loss_object, steps):\n",
    "\n",
    "  #Tiempo de inicio\n",
    "  start = time.time()\n",
    "\n",
    "  #Se itera y se cogen N steps de imágenes\n",
    "  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
    "\n",
    "    # Limpia la pantalla cuando hay 1k pasos e imprime el tiempo que ha tardado\n",
    "    with tf.device('/CPU:0'):\n",
    "      if (step) % 1000 == 0:\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        if step != 0:\n",
    "          print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "        start = time.time()\n",
    "        print(f\"Step: {step//1000}k\")\n",
    "\n",
    "    #Entrenamiento de los modelos\n",
    "    train_step(generator, discriminator,generator_optimizer, discriminator_optimizer,loss_object,input_image,target)\n",
    "\n",
    "    # Cada 10 pasos pone un punto\n",
    "    with tf.device('/CPU:0'):\n",
    "      if (step+1) % 10 == 0:\n",
    "        print('.', end='', flush=True)\n",
    "\n",
    "\n",
    "    # Guarda el checkpoint cada 5k\n",
    "    with tf.device('/CPU:0'):\n",
    "      if (step + 1) % 5000 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Entrenamiento y comprobación del modelo <a id=\"ganecm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5.1 Entrenamiento de 256->1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un checkpoint en caso de que se quiera reaunudar el entrenamiento o se quiera recuperar una etapa anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer_256 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer_256 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "loss_object_256 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/GAN/ckpt_gan_256/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint_256 = tf.train.Checkpoint(generator_optimizer=generator_optimizer_256,\n",
    "                                 discriminator_optimizer=discriminator_optimizer_256,\n",
    "                                 generator= generator_256,\n",
    "                                 discriminator= discriminator_256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrena el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(generator_256,discriminator_256,low_res_train_dataset,generator_optimizer_256,discriminator_optimizer_256,checkpoint_256,checkpoint_prefix,loss_object_256,40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_256.save(\"Checkpoints/GAN/generator_256_model\")\n",
    "discriminator_256.save(\"Checkpoints/GAN/discriminator_256_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra el resultado del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(generator_256,low_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5.2 Entrenamiento de 512->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer_512 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer_512 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "loss_object_512 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/GAN/ckpt_gan_512/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint_512 = tf.train.Checkpoint(generator_optimizer=generator_optimizer_512,\n",
    "                                 discriminator_optimizer=discriminator_optimizer_512,\n",
    "                                 generator= generator_512,\n",
    "                                 discriminator= discriminator_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(generator_512,discriminator_512,med_res_train_dataset,generator_optimizer_512,discriminator_optimizer_512,checkpoint_512,checkpoint_prefix,loss_object_512,30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_512.save(\"Checkpoints/GAN/generator_512_model\")\n",
    "discriminator_512.save(\"Checkpoints/GAN/discriminator_512_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(generator_512,med_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5.3 Entrenamiento de 1024->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer_1024 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer_1024 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "loss_object_1024 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/GAN/ckpt_gan_1024/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint_1024 = tf.train.Checkpoint(generator_optimizer=generator_optimizer_1024,\n",
    "                                 discriminator_optimizer=discriminator_optimizer_1024,\n",
    "                                 generator= generator_1024,\n",
    "                                 discriminator= discriminator_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(generator_1024,discriminator_1024,high_res_train_dataset,generator_optimizer_1024,discriminator_optimizer_1024,checkpoint_1024,checkpoint_prefix,loss_object_1024,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_1024.save(\"Checkpoints/GAN/generator_1024_model\")\n",
    "discriminator_1024.save(\"Checkpoints/GAN/discriminator_1024_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(generator_1024,high_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, este modelo no consigue mantener el color y con algunos errores en la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 GAN (U-Net o cGAN o pix2pix) <a id=\"cgan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.4.1 Declaración de los Generadores <a id=\"dunet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta Red GAN está basada en el documento pix2pix y su implementación en la documentación de tensorflow.\n",
    "\n",
    "Lo único que cambia respecto de las anteriores redes es el generador. Los generadores son una U-NET un poco modificada:\n",
    "\n",
    "- El modelo de x4 tiene 7 capas de downsample y 6 capas de upsample junto con una capa extra de Conv2DTranspose y una operación de profundidad a espacio.\n",
    "- El modelo de x2 tiene 8 capas de downsample y 7 capas de upsample junto con una operación de profundidad a espacio.\n",
    "- El modelo de x1 tiene 9 capas de downsample y 8 capas de upsample sin operaciones extra.\n",
    "\n",
    "\n",
    "![resumen U-NET](./Images/Resumen_U-NET.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.1 Generador 256 -> 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_Generator_256():\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "\n",
    "  #Downsample\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "  ]\n",
    "\n",
    "  #Upsample\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "    upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "    upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "    upsample(64, 4),  # (batch_size, 128, 128, 64)\n",
    "  ]\n",
    "\n",
    "  # Última capa de convolucion:\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(3, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Conecta las capas de downsample:\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Conecta y concatena las capas de downsample con las de upsample\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  #Escalado del modelo:\n",
    "  x = tf.keras.layers.Conv2DTranspose(32, 4, strides=2, padding='same', kernel_initializer=initializer, use_bias=False)(x)\n",
    "  x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='SAME')(x)\n",
    "  x = tf.keras.layers.Lambda(lambda x : tf.nn.depth_to_space(x,2))(x)\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_256 = unet_Generator_256()\n",
    "tf.keras.utils.plot_model(unet_generator_256, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.2 Generador 512 -> 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_Generator_512():\n",
    "  inputs = tf.keras.layers.Input(shape=[512, 512, 3])\n",
    "\n",
    "  #Downsample\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 256, 256, 64)\n",
    "    downsample(128, 4),  # (batch_size, 128, 128, 128)\n",
    "    downsample(256, 4),  # (batch_size, 64, 64, 256)\n",
    "    downsample(512, 4),  # (batch_size, 32, 32, 512)\n",
    "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "  ]\n",
    "\n",
    "  #Upsample\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 32, 32, 1024)\n",
    "    upsample(256, 4),  # (batch_size, 64, 64, 512)\n",
    "    upsample(128, 4),  # (batch_size, 128, 128, 256)\n",
    "    upsample(64, 4),  # (batch_size, 256, 256, 128)\n",
    "  ]\n",
    "\n",
    "  # Última capa de convolucion:\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(3, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Conecta las capas de downsample:\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Conecta y concatena las capas de downsample con las de upsample\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  #Escalado del modelo:\n",
    "  x = tf.keras.layers.Conv2D(filters= (512 * 4), kernel_size=3, strides=1, padding='SAME')(x)\n",
    "  x = tf.keras.layers.Lambda(lambda x : tf.nn.depth_to_space(x,2))(x)\n",
    "\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_512 = unet_Generator_512()\n",
    "tf.keras.utils.plot_model(unet_generator_512, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1.3 Generador 1024->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_Generator_1024():\n",
    "  inputs = tf.keras.layers.Input(shape=[1024, 1024, 3])\n",
    "\n",
    "  #Downsample\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 512, 512, 64)\n",
    "    downsample(128, 4),  # (batch_size, 256, 256, 128)\n",
    "    downsample(256, 4),  # (batch_size, 128, 128, 256)\n",
    "    downsample(512, 4),  # (batch_size, 64, 64, 512)\n",
    "    downsample(512, 4),  # (batch_size, 32, 32, 512)\n",
    "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "  ]\n",
    "\n",
    "  #Upsample\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 16, 8, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 32, 32, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 64, 64, 1024)\n",
    "    upsample(256, 4),  # (batch_size, 128, 128, 512)\n",
    "    upsample(128, 4),  # (batch_size, 256, 256, 256)\n",
    "    upsample(64, 4),  # (batch_size, 512, 512, 128)\n",
    "  ]\n",
    "\n",
    "  # Última capa de convolucion:\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(3, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Conecta las capas de downsample:\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Conecta y concatena las capas de downsample con las de upsample\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_1024 = unet_Generator_1024()\n",
    "tf.keras.utils.plot_model(unet_generator_1024, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Declaración de los discriminadores y entrenamiento <a id=\"unetde\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_discriminator_256 = Discriminator()\n",
    "unet_discriminator_512 = Discriminator()\n",
    "unet_discriminator_1024 = Discriminator()\n",
    "\n",
    "tf.keras.utils.plot_model(unet_discriminator_256, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2.1 Entrenamiento de 256->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_optimizer_256 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "unet_discriminator_optimizer_256 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "unet_loss_object_256 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/unet_GAN/ckpt_gan_256/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "unet_checkpoint_256 = tf.train.Checkpoint(generator_optimizer=unet_generator_optimizer_256,\n",
    "                                 discriminator_optimizer=unet_discriminator_optimizer_256,\n",
    "                                 generator=unet_generator_256,\n",
    "                                 discriminator=unet_discriminator_256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(unet_generator_256,unet_discriminator_256,low_res_train_dataset,unet_generator_optimizer_256,unet_discriminator_optimizer_256,unet_checkpoint_256,checkpoint_prefix,unet_loss_object_256,40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_256.save(\"Checkpoints/unet_GAN/generator_256_model\")\n",
    "unet_discriminator_256.save(\"Checkpoints/unet_GAN/discriminator_256_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(unet_generator_256,low_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2.2 Entrenamiento de 512->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_optimizer_512 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "unet_discriminator_optimizer_512 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "unet_loss_object_512 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/unet_GAN/ckpt_gan_512/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "unet_checkpoint_512 = tf.train.Checkpoint(generator_optimizer=unet_generator_optimizer_512,\n",
    "                                 discriminator_optimizer=unet_discriminator_optimizer_512,\n",
    "                                 generator=unet_generator_512,\n",
    "                                 discriminator=unet_discriminator_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(unet_generator_512,unet_discriminator_512,med_res_train_dataset,unet_generator_optimizer_512,unet_discriminator_optimizer_512,unet_checkpoint_512,checkpoint_prefix,unet_loss_object_512,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_512.save(\"Checkpoints/unet_GAN/generator_512_model\")\n",
    "unet_discriminator_512.save(\"Checkpoints/unet_GAN/discriminator_512_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(unet_generator_512,med_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2.1 Entrenamiento de 1024->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_optimizer_1024 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "unet_discriminator_optimizer_1024 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "unet_loss_object_1024 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/unet_GAN/ckpt_gan_1024/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "unet_checkpoint_1024 = tf.train.Checkpoint(generator_optimizer=unet_generator_optimizer_1024,\n",
    "                                 discriminator_optimizer=unet_discriminator_optimizer_1024,\n",
    "                                 generator=unet_generator_1024,\n",
    "                                 discriminator=unet_discriminator_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(unet_generator_1024,unet_discriminator_1024,high_res_train_dataset,unet_generator_optimizer_1024,unet_discriminator_optimizer_1024,unet_checkpoint_1024,checkpoint_prefix,unet_loss_object_1024,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_generator_1024.save(\"Checkpoints/unet_GAN/generator_1024_model\")\n",
    "unet_discriminator_1024.save(\"Checkpoints/unet_GAN/discriminator_1024_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(unet_generator_1024,high_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo lo hace mejor que los anteriores, pero sigue dejando un poco de ruido en la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 EDSR_GAN <a id=\"edsrgan\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Creación del generador con arquitectura EDSR <a id=\"edsr\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la arquitectura EDSR los bloques residuales NO tienen batchnormalization ya que le puede quitar calidad a la imagen.\n",
    "\n",
    "Este tipo de arquitextura consta de B bloques residuales con F filtros por capa de convolución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/EDSR_structure.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Imagen sacada de el paper: https://arxiv.org/pdf/1707.02921"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EDSR(scale, input_shape,B=8, F=256, scale_factor=0.1):\n",
    "\n",
    "    xinput = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Primer Conv2d\n",
    "    xlast = x = tf.keras.layers.Conv2D(filters=F, kernel_size=3, strides=1, padding='SAME')(xinput)\n",
    "    \n",
    "    # Bloques residuales:\n",
    "    def res_block(x):\n",
    "        x1 = tf.keras.layers.Conv2D(filters= F, kernel_size=3, strides=1, padding='SAME', activation=\"relu\")(x) # Conv2d + relu\n",
    "        x2 = tf.keras.layers.Conv2D(filters= F, kernel_size=3, strides=1, padding='SAME')(x1) # Conv2d\n",
    "\n",
    "        x2 = x2 * scale_factor # Mult\n",
    "        output = tf.keras.layers.Add()([x2, x]) # Add\n",
    "        return output\n",
    "    \n",
    "    # B ResBlocks\n",
    "    for i in range(B):\n",
    "        x = res_block(x)\n",
    "\n",
    "    # Último Conv2d\n",
    "    x = tf.keras.layers.Conv2D(filters=F, kernel_size=3, strides=1, padding='SAME')(x)\n",
    "    # Último Add\n",
    "    x = tf.keras.layers.Add()([x,xlast])\n",
    "\n",
    "    # Upsample\n",
    "    if scale == 2: # x2 512\n",
    "        x = tf.keras.layers.Conv2D(filters= (16), kernel_size=3, strides=1, padding='SAME')(x)\n",
    "        x = tf.keras.layers.Lambda(lambda x : tf.nn.depth_to_space(x,2))(x)\n",
    "    elif scale == 4: # x4 256\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters= (32), kernel_size=3, strides=1, padding='SAME')(x)\n",
    "        x = tf.keras.layers.Lambda(lambda x : tf.nn.depth_to_space(x,2))(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters= (16), kernel_size=3, strides=1, padding='SAME')(x)\n",
    "        x = tf.keras.layers.Lambda(lambda x : tf.nn.depth_to_space(x,2))(x)\n",
    "\n",
    "    out = tf.keras.layers.Conv2D(filters=3, kernel_size=3, strides=1, padding='SAME')(x)\n",
    "\n",
    "    return tf.keras.models.Model(xinput, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDSR_x1(scale, input_shape,B=8, F=256, scale_factor=0.1):\n",
    "\n",
    "    xinput = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    #Downsample\n",
    "    x = downsample(16,4,apply_batchnorm=False)(xinput)\n",
    "    x = downsample(32,4,apply_batchnorm=False)(x)\n",
    "\n",
    "    xlast = x = tf.keras.layers.Conv2D(filters=F, kernel_size=3, strides=1, padding='SAME')(x)\n",
    "    \n",
    "    # Bloques residuales:\n",
    "    def res_block(x):\n",
    "        x1 = tf.keras.layers.Conv2D(filters= F, kernel_size=3, strides=1, padding='SAME', activation=\"relu\")(x) # Conv2d + relu\n",
    "        x2 = tf.keras.layers.Conv2D(filters= F, kernel_size=3, strides=1, padding='SAME')(x1) # Conv2d\n",
    "\n",
    "        x2 = x2 * scale_factor # Mult\n",
    "        output = tf.keras.layers.Add()([x2, x]) # Add\n",
    "        return output\n",
    "    \n",
    "    # B ResBlocks\n",
    "    for i in range(B):\n",
    "        x = res_block(x)\n",
    "\n",
    "    # Último Conv2d\n",
    "    x = tf.keras.layers.Conv2D(filters=F, kernel_size=3, strides=1, padding='SAME')(x)\n",
    "    # Último Add\n",
    "    x = tf.keras.layers.Add()([x,xlast])\n",
    "\n",
    "    # Upsample\n",
    "\n",
    "    x = upsample(32,4)(x)\n",
    "    x = upsample(16,4)(x)\n",
    "\n",
    "    out = tf.keras.layers.Conv2D(filters=3, kernel_size=3, strides=1, padding='SAME')(x)\n",
    "\n",
    "    return tf.keras.models.Model(xinput, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_256 = EDSR(4,(256,256,3))\n",
    "tf.keras.utils.plot_model(edsr_generator_256, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_512 = EDSR(2,(512,512,3),B=8,F=128)\n",
    "tf.keras.utils.plot_model(edsr_generator_512, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_1024 = EDSR_x1(1,(1024,1024,3),B=8,F=256)\n",
    "tf.keras.utils.plot_model(edsr_generator_1024, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Declaración de los discriminadores y entrenamiento <a id=\"edsrde\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_discriminator_256 = Discriminator()\n",
    "edsr_discriminator_512 = Discriminator()\n",
    "edsr_discriminator_1024 = Discriminator()\n",
    "\n",
    "#tf.keras.utils.plot_model(edsr_discriminator_256, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2.1 Entrenamiento 256->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_optimizer_256 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "edsr_discriminator_optimizer_256 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "edsr_loss_object_256 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/edsr_GAN/ckpt_gan_256/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "edsr_checkpoint_256 = tf.train.Checkpoint(generator_optimizer=edsr_generator_optimizer_256,\n",
    "                                 discriminator_optimizer=edsr_discriminator_optimizer_256,\n",
    "                                 generator=edsr_generator_256,\n",
    "                                 discriminator=edsr_discriminator_256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(edsr_generator_256,edsr_discriminator_256,low_res_train_dataset,edsr_generator_optimizer_256,edsr_discriminator_optimizer_256,edsr_checkpoint_256,checkpoint_prefix,edsr_loss_object_256,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_256.save(\"Checkpoints/edsr_GAN/generator_256_model\")\n",
    "edsr_discriminator_256.save(\"Checkpoints/edsr_GAN/discriminator_256_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(edsr_generator_256,low_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2.2 Entrenamiento 512->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_optimizer_512 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "edsr_discriminator_optimizer_512 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "edsr_loss_object_512 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/edsr_GAN/ckpt_gan_512/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "edsr_checkpoint_512 = tf.train.Checkpoint(generator_optimizer=edsr_generator_optimizer_512,\n",
    "                                 discriminator_optimizer=edsr_discriminator_optimizer_512,\n",
    "                                 generator=edsr_generator_512,\n",
    "                                 discriminator=edsr_discriminator_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(edsr_generator_512,edsr_discriminator_512,med_res_train_dataset,edsr_generator_optimizer_512,edsr_discriminator_optimizer_512,edsr_checkpoint_512,checkpoint_prefix,edsr_loss_object_512,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_512.save(\"Checkpoints/edsr_GAN/generator_512_model\")\n",
    "edsr_discriminator_512.save(\"Checkpoints/edsr_GAN/discriminator_512_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(edsr_generator_512,med_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.2.3 Entrenamiento 1024->1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_optimizer_1024 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "edsr_discriminator_optimizer_1024 = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "edsr_loss_object_1024 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "checkpoint_dir = './Checkpoints/edsr_GAN/ckpt_gan_1024/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "edsr_checkpoint_1024 = tf.train.Checkpoint(generator_optimizer=edsr_generator_optimizer_1024,\n",
    "                                 discriminator_optimizer=edsr_discriminator_optimizer_1024,\n",
    "                                 generator=edsr_generator_1024,\n",
    "                                 discriminator=edsr_discriminator_1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(edsr_generator_1024,edsr_discriminator_1024,high_res_train_dataset,edsr_generator_optimizer_1024,edsr_discriminator_optimizer_1024,edsr_checkpoint_1024,checkpoint_prefix,edsr_loss_object_1024,15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_generator_1024.save(\"Checkpoints/edsr_GAN/generator_1024_model\")\n",
    "edsr_discriminator_1024.save(\"Checkpoints/edsr_GAN/discriminator_1024_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_diff(edsr_generator_1024,high_res_valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, este modelo funciona muy bien con las imágenes de x2 y x4, pero con x1, al reducir la dimensionalidad del modelo, se pierde mucha información y por eso pierde colores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparación de modelos <a id=\"comparacionmodelos\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va ha proceder a analizar los resultados de entrenamiento de cada modelo separados por tres categorías:\n",
    "\n",
    "- 256x256 a 1024x1024 (resolución x4)\n",
    "- 512x512 a 1024x1024 (resolución x2)\n",
    "- 1024x1024 a 1024x1024 (resolución x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se cargan todos los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Carga de modelos: <a id=\"cm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_256 = tf.keras.models.load_model('Checkpoints\\Autoencoder/autoencoder_256_model')\n",
    "\n",
    "autoencoder_512 = tf.keras.models.load_model('Checkpoints\\Autoencoder/autoencoder_512_model')\n",
    "\n",
    "autoencoder_1024 = tf.keras.models.load_model('Checkpoints\\Autoencoder/autoencoder_1024_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(latent_dim):\n",
    "    @tf.function\n",
    "    def _sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
    "        return z_mean + tf.keras.backend.exp(z_log_var / 2) * epsilon\n",
    "    return _sampling\n",
    "\n",
    "\n",
    "custom_object_vae = {'sampling': sampling(256)}\n",
    "\n",
    "custom_object_vae_pathc = {'sampling': sampling(200)}\n",
    "\n",
    "\n",
    "#Ambos modelos son de 1024x1024\n",
    "vae = tf.keras.models.load_model('Checkpoints/VAE/VAE_1024_model',custom_objects=custom_object_vae)\n",
    "\n",
    "vae_patch = tf.keras.models.load_model('Checkpoints\\VAE_PATCH\\VAE_64_model',custom_objects=custom_object_vae_pathc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_256 = tf.keras.models.load_model('Checkpoints\\GAN\\generator_256_model')\n",
    "\n",
    "gan_512 = tf.keras.models.load_model('Checkpoints\\GAN\\generator_512_model')\n",
    "\n",
    "gan_1024 = tf.keras.models.load_model('Checkpoints\\GAN\\generator_1024_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN U-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_gan_256 = tf.keras.models.load_model(r'Checkpoints\\unet_GAN\\generator_256_model')\n",
    "\n",
    "unet_gan_512 = tf.keras.models.load_model(r'Checkpoints\\unet_GAN\\generator_512_model')\n",
    "\n",
    "unet_gan_1024 = tf.keras.models.load_model(r'Checkpoints\\unet_GAN\\generator_1024_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_gan_256 = tf.keras.models.load_model(r'Checkpoints\\edsr_GAN\\generator_256_model')\n",
    "\n",
    "edsr_gan_512 = tf.keras.models.load_model(r'Checkpoints\\edsr_GAN\\generator_512_model')\n",
    "\n",
    "edsr_gan_1024 = tf.keras.models.load_model(r'Checkpoints\\edsr_GAN\\generator_1024_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Comparativa de imágenes <a id=\"ci\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va ha proceder a comparar visualmente los resultados de cada modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"autoencoder\",\"gan\",\"unet_gan\",\"edsr_gan\"]\n",
    "model_list_256 = [autoencoder_256,gan_256,unet_gan_256,edsr_gan_256]\n",
    "model_list_512 = [autoencoder_512,gan_512,unet_gan_512,edsr_gan_512]\n",
    "model_list_1024 = [autoencoder_1024,gan_1024,unet_gan_1024,edsr_gan_1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Imágenes de 256x256 a 1024x1024 (resolución x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo la siguiente imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_256, target_256 = next(iter(low_res_valid_dataset))\n",
    "plot_dataset(input=input_256.numpy(),target=target_256.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model,title in zip(model_list_256,names):\n",
    "    plot_model_diff(model=model,input=input_256,target=target_256,figsize=(12,12),title=str(title+\"_256\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Imágenes de 512x512 a 1024x1024 (resolución x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo la siguiente imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_512, target_512 = next(iter(med_res_valid_dataset))\n",
    "plot_dataset(input=input_512.numpy(),target=target_512.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model,title in zip(model_list_512,names):\n",
    "    plot_model_diff(model=model,input=input_512,target=target_512,figsize=(12,12),title=str(title+\"_512\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Imágenes de 1024x1024 a 1024x1024 (resolución x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo la siguiente imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1024, target_1024 = next(iter(high_res_valid_dataset))\n",
    "plot_dataset(input=input_1024.numpy(),target=target_1024.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_patch_reconstruct(vae_patch,input_1024,figsize=(8,8),title=\"PATCH_VAE\")\n",
    "\n",
    "for model,title in zip(model_list_1024,names):\n",
    "    plot_model_diff(model=model,input=input_1024,target=target_1024,figsize=(12,12),title=str(title+\"_1024\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Tablas comparativas de métricas <a id=\"tcm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que, pasándole una imagen y un modelo, la divide en parches, predice cada parche y reconstruye la imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_predict(model,single_image):\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=single_image,  \n",
    "        sizes=[1, 64, 64, 1], \n",
    "        strides=[1, 64, 64, 1], \n",
    "        rates=[1, 1, 1, 1], \n",
    "        padding='VALID'  \n",
    "    )\n",
    "\n",
    "    patches = tf.reshape(patches, [-1, 64, 64, 3])\n",
    "\n",
    "    reconstructed_patches = model.predict(patches,verbose=0)\n",
    "\n",
    "    reconstructed_image = np.zeros((1024, 1024, 3),dtype=np.float32)\n",
    "\n",
    "    # Coloca cada parche en la imagen reconstruida\n",
    "    patch_size = 64\n",
    "    num_patches_per_row = 1024 // patch_size\n",
    "    for i in range(len(reconstructed_patches)):\n",
    "        row = i // num_patches_per_row\n",
    "        col = i % num_patches_per_row\n",
    "        reconstructed_image[row*patch_size:(row+1)*patch_size, col*patch_size:(col+1)*patch_size, :] = reconstructed_patches[i]\n",
    "    return reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        'Model': [],\n",
    "        'PSNR_x1': [],\n",
    "        'PSNR_x2': [],\n",
    "        'PSNR_x4': [],\n",
    "        'SSIM_x1' :[],\n",
    "        'SSIM_x2' :[],\n",
    "        'SSIM_x4' :[],\n",
    "        'mae_x1' :[],\n",
    "        'mae_x2' :[],\n",
    "        'mae_x4' :[],\n",
    "        'mse_x1' :[],\n",
    "        'mse_x2' :[],\n",
    "        'mse_x4' :[],\n",
    "        }\n",
    "df_one_image = pd.DataFrame(data)\n",
    "df_mean = pd.DataFrame(data)\n",
    "\n",
    "input_list = [input_256,input_512,input_1024]\n",
    "target_list = [target_256,target_512,target_1024]\n",
    "\n",
    "autoencoder_list = [autoencoder_256,autoencoder_512,autoencoder_1024]\n",
    "gan_list = [gan_256,gan_512,gan_1024]\n",
    "unet_gan_list = [unet_gan_256,unet_gan_512,unet_gan_1024]\n",
    "edsr_gan_list = [edsr_gan_256,edsr_gan_512,edsr_gan_1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row(df,input_list,target_list,model_list,model_name,i):\n",
    "\n",
    "    df.loc[i,'Model'] = model_name\n",
    "\n",
    "    l = [4,2,1]\n",
    "    if model_name == \"VAE\":\n",
    "        l = [1]\n",
    "        input_list = [input_list[2]]\n",
    "        target_list = [target_list[2]]\n",
    "    \n",
    "\n",
    "    for j, input, target, model in zip(l,input_list, target_list, model_list):\n",
    "        \n",
    "        if model_name != \"VAE\":\n",
    "            prediction = model.predict(input, verbose=0)[0]\n",
    "        else:\n",
    "            prediction = patch_predict(model,input)\n",
    "            \n",
    "        prediction = (prediction - np.min(prediction)) / (np.max(prediction) - np.min(prediction))\n",
    "\n",
    "        # Calcular métricas\n",
    "        psnr_val = tf.image.psnr(prediction, target, max_val=1)\n",
    "        ssim_val = tf.image.ssim(prediction, target, max_val=1)\n",
    "        mae_val = tf.keras.losses.mae(prediction, target)\n",
    "        mse_val = tf.keras.losses.mse(prediction, target)\n",
    "\n",
    "        #Asignar valores\n",
    "        df.loc[i, f'PSNR_x{j}'] = psnr_val.numpy()\n",
    "        df.loc[i, f'SSIM_x{j}'] = ssim_val.numpy()\n",
    "        df.loc[i, f'mae_x{j}'] = np.mean(mae_val.numpy())\n",
    "        df.loc[i, f'mse_x{j}'] = np.mean(mse_val.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_row(df_one_image,input_list,target_list,autoencoder_list,\"Autoencoder\",0)\n",
    "add_row(df_one_image,input_list,target_list,[vae_patch],\"VAE\",1)\n",
    "add_row(df_one_image,input_list,target_list,gan_list,\"GAN\",2)\n",
    "add_row(df_one_image,input_list,target_list,unet_gan_list,\"GAN UNET\",3)\n",
    "add_row(df_one_image,input_list,target_list,edsr_gan_list,\"GAN EDSR\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_image.to_excel(\"metrics_one_image.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_mean(df, dataset_list,model_list, model_name, i):\n",
    "\n",
    "    df.loc[i, 'Model'] = model_name\n",
    "    l = [4, 2, 1]\n",
    "\n",
    "    if model_name == \"VAE\":\n",
    "        l = [1]\n",
    "\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "    mae_values = []\n",
    "    mse_values = []\n",
    "\n",
    "    for j,dataset, model in zip(l,dataset_list, model_list):\n",
    "        for input, target in dataset.as_numpy_iterator():\n",
    "            if model_name != \"VAE\":\n",
    "                prediction = model.predict(input, verbose=0)[0]\n",
    "            else:\n",
    "                prediction = patch_predict(model, input)\n",
    "\n",
    "            prediction = (prediction - np.min(prediction)) / (np.max(prediction) - np.min(prediction))\n",
    "\n",
    "            # Calcular métricas\n",
    "            psnr_val = tf.image.psnr(prediction, target, max_val=1)\n",
    "            ssim_val = tf.image.ssim(prediction, target, max_val=1)\n",
    "            mae_val = tf.keras.losses.mae(prediction, target)\n",
    "            mse_val = tf.keras.losses.mse(prediction, target)\n",
    "\n",
    "            # Guardar valores\n",
    "            psnr_values.append(psnr_val.numpy())\n",
    "            ssim_values.append(ssim_val.numpy())\n",
    "            mae_values.append(np.mean(mae_val.numpy()))\n",
    "            mse_values.append(np.mean(mse_val.numpy()))\n",
    "\n",
    "        df.loc[i, f'PSNR_x{j}'] = np.mean(psnr_values)\n",
    "        df.loc[i, f'SSIM_x{j}'] = np.mean(ssim_values)\n",
    "        df.loc[i, f'mae_x{j}'] = np.mean(mae_values)\n",
    "        df.loc[i, f'mse_x{j}'] = np.mean(mse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [low_res_valid_dataset,med_res_valid_dataset,high_res_valid_dataset]\n",
    "\n",
    "add_row_mean(df_mean,dataset_list,autoencoder_list,\"Autoencoder\",0)\n",
    "add_row_mean(df_mean,[high_res_valid_dataset],[vae_patch],\"VAE\",1)\n",
    "add_row_mean(df_mean,dataset_list,gan_list,\"GAN\",2)\n",
    "add_row_mean(df_mean,dataset_list,unet_gan_list,\"GAN UNET\",3)\n",
    "add_row_mean(df_mean,dataset_list,edsr_gan_list,\"GAN EDSR\",4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean.to_excel(\"metrics_mean.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
